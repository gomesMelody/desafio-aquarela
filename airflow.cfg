[core]
# The home directory for Airflow
airflow_home = /opt/airflow
# The SQLite database URI (this can be replaced by Postgres or MySQL if needed)
sql_alchemy_conn = sqlite:////opt/airflow/airflow.db
# The executor for running tasks (LocalExecutor is common for simple setups)
executor = LocalExecutor
# Whether to use a scheduler
scheduler = True
# The default timezone used by Airflow
default_timezone = UTC

[logging]
# The folder where logs are stored
base_log_folder = /opt/airflow/logs
# Whether remote logging is enabled (set to False for local logging)
remote_logging = False
# The logging level for Airflow's logs (e.g., INFO, DEBUG)
log_level = INFO
# The logging format for logs
log_format = %(asctime)s - %(name)s - %(levelname)s - %(message)s
# The logging date format for logs
log_date_format = %Y-%m-%d %H:%M:%S

[webserver]
# The webserver port
web_server_port = 8080
# Whether to use SSL for the webserver
web_server_ssl_cert = 
web_server_ssl_key = 
# Whether to enable RBAC (Role-Based Access Control)
rbac = True
# Secret key for the webserver (you can generate a random secret key)
secret_key = CHANGE_THIS_TO_A_SECRET_KEY

[scheduler]
# The scheduler settings, including whether to run in parallel
scheduler_task_queued_timeout = 300
scheduler_task_queued_poll_interval = 5

[database]
# The connection string for the database
# For Postgres or MySQL, replace this string accordingly
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
# The connection string for the metadata database (used by Airflow)
sql_alchemy_pool_size = 5
sql_alchemy_pool_recycle = 3600

[celery]
# Settings for the Celery executor (if you decide to use Celery for distributed task execution)
broker_url = redis://redis:6379/0
result_backend = redis://redis:6379/0
